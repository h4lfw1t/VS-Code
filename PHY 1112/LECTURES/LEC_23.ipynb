{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization and Gradient Descent\n",
    "\n",
    "Recall that the optimization of a function $f{x}$ is the process of finding the minimum or maximum value of $f(x)$. Typically, this is done by using the derivative to find the maximum or minimum points of the function (the roots of the derivative).\n",
    "\n",
    "A gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In other words, it is an iterative algorithm used to find the minimum of a function.\n",
    "Gradient descent uses the gradient of the function, defined as:\n",
    "\n",
    "$$\\nabla f(x) = \\left[ \\frac{\\partial f}{\\partial x_1}\\hat{x_1}, \\frac{\\partial f}{\\partial x_2}\\hat{x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n}\\hat{x_n} \\right]$$\n",
    "\n",
    "$\\nabla f(x)$ is a vector that points in the direction of the greatest rate of increase of the function at point $x$. The negative of the gradient points in the direction of the greatest rate of decrease of the function at point $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets say $f(x_1, x_2) = x_1^2 + x_2^2$. The gradient of $f(x_1, x_2)$ is:\n",
    "\n",
    "$$\\nabla f(x_1, x_2) = \\left[ \\frac{\\partial f}{\\partial x_1}\\hat{x_1}, \\frac{\\partial f}{\\partial x_2}\\hat{x_2} \\right] = \\left[ 2x_1, 2x_2 \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus at the point $(x_1, x_2) = (1, 1)$, the gradient is equal to $(2\\hat{x_1}, 2\\hat{x_2})$. If we take the negative of this, notice that the negative of the gradient points in the direction of the greatest rate of decrease of the function at point $(1, 1)$.\n",
    "Thus if we take a step in this negative direction, we will be moving towards the minimum of the function. The new location will be the current location with a step size $\\alpha$ in the negative direction of the gradient. It thus depends on the step size $\\alpha$ how fast we converge to the minimum.\n",
    "\n",
    "In vector form, with $\\vec{x} = (x_1, x_2)$, the update rule is:\n",
    "\n",
    "$$\\vec{x_new} = \\vec{x_current} - \\gamma \\nabla f(\\vec{x_current})$$\n",
    "\n",
    "where $\\gamma$ is the step size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_{1, new} = x_{1, current} - \\gamma \\frac{\\partial f(x_{1, current}, x_{2, current})}{\\partial x_1} \\\\ \\\\\n",
    "x_{2, new} = x_{2, current} - \\gamma \\frac{\\partial f(x_{1, current}, x_{2, current})}{\\partial x_2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we don't know the function we are trying to minimize, but we can estimate each partial derivative using finite differences. The finite difference approximation of the partial derivative of $f(x)$ with respect to $x_i$ is:\n",
    "\n",
    "(forward difference)\n",
    "$$\\frac{\\partial f(x_1, x_2, x_3, \\ldots, x_i, \\ldots, x_n)}{\\partial x_i} \\approx \\frac{f(x_1, x_2, x_3, \\ldots, x_i + h_i, \\ldots, x_n) - f(x_1, x_2, x_3, \\ldots, x_i, \\ldots, x_n)}{h}$$\n",
    "\n",
    "(backward differece)\n",
    "$$\\frac{\\partial f(x_1, x_2, x_3, \\ldots, x_i, \\ldots, x_n)}{\\partial x_i} \\approx \\frac{f(x_1, x_2, x_3, \\ldots, x_i, \\ldots, x_n) - f(x_1, x_2, x_3, \\ldots, x_i - h_i, \\ldots, x_n)}{h}$$\n",
    "\n",
    "(central difference)\n",
    "$$\\frac{\\partial f(x_1, x_2, x_3, \\ldots, x_i, \\ldots, x_n)}{\\partial x_i} \\approx \\frac{f(x_1, x_2, x_3, \\ldots, x_i + \\frac{h_i}{2}, \\ldots, x_n) - f(x_1, x_2, x_3, \\ldots, x_i - \\frac{h_i}{2}, \\ldots, x_n)}{2h}$$\n",
    "\n",
    "where $h$ is a small number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine when to stop the algorithm, we can use a stopping criterion.\n",
    "1. Stop when the norm of the gradient is less than a threshold $\\epsilon$. That is, stop when $|\\nabla f(x)| < \\epsilon$.\n",
    "2. Stop when the function is converging to the same value. That is, stop when $|f(x_{new}) - f(x_{current})| < \\epsilon$.\n",
    "3. Stop when the point $(x_1, x_2, \\ldots, x_n)$ is converging to the same location. That is stop when $|x_{new} - x_{current}| < \\epsilon$.\n",
    "\n",
    "Recall that $\\epsilon$ is referred to as the tolerance.\n",
    "\n",
    "It is also a good idea to set a maximum number of iterations to prevent the algorithm from running indefinitely.\n",
    "\n",
    "Below is an example of how to implement the gradient descent algorithm in Python for a single variable function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4988115775632638"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(f, x, h, gamma, tol, max_iter):\n",
    "    '''\n",
    "    (function, float, float, float, float, int) -> float\n",
    "    This function finds the minimum of a function using the gradient descent method for a single variable function.\n",
    "    Preconditions: f is a single variable function, tol > 0, max_iter > 0\n",
    "    '''\n",
    "    for _ in range(max_iter):                                       # iterate through the maximum number of iterations\n",
    "        grad = (f(x + h / 2) - f(x - h / 2)) / h                                # calculate the gradient for f(x) at x\n",
    "        x -= gamma * grad                                           # update x\n",
    "        # if np.linalg.norm(grad) < tol:                            # if the norm of the gradient is less than the tolerance, break (method 1)\n",
    "        if np.linalg.norm(f(x) - f(x + gamma * grad)) < tol:        # if the difference between f(x_new) and f(x_current) is less than the tolerance, break (method 2)\n",
    "            break\n",
    "        if _ == max_iter - 1:                                       # if the maximum number of iterations is reached, print a message\n",
    "            print(\"Max iterations reached\")\n",
    "    return x                                                        # return the minimum of the function\n",
    "\n",
    "def f(x):\n",
    "    return x**2 + x\n",
    "\n",
    "gradient_descent(f, 1, 1e-6, 0.1, 1e-6, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A gradient descent for a function of two variables is also implemented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0007922816251565981, 0.0007922816251565981)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def two_variable_gradient_descent(f, x, y, h, gamma, tol, max_iter):\n",
    "    '''\n",
    "    (function, float, float, float, float, float, int) -> float\n",
    "    This function finds the minimum of a function using the gradient descent method for a two variable function.\n",
    "    Preconditions: f is a two variable function, tol > 0, max_iter > 0\n",
    "    '''\n",
    "    for _ in range(max_iter):\n",
    "        grad_x = (f(x + h / 2, y) - f(x - h / 2, y)) / h\n",
    "        grad_y = (f(x, y + h / 2) - f(x, y - h / 2)) / h\n",
    "        x -= gamma * grad_x\n",
    "        y -= gamma * grad_y\n",
    "        # if np.linalg.norm([grad_x, grad_y]) < tol:\n",
    "        if np.linalg.norm(f(x, y) - f(x + gamma * grad_x, y + gamma * grad_y)) < tol:\n",
    "            break\n",
    "        if _ == max_iter - 1:\n",
    "            print(\"Max iterations reached\")\n",
    "    return x, y\n",
    "\n",
    "def f(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "two_variable_gradient_descent(f, 1, 1, 1e-6, 0.1, 1e-6, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above function, it is easy to see that for every additional variable, the gradient descent algorithm will require an additional partial derivative (grad) which must then be implemented into the if statement. Thus for a function of $n$ variables, the gradient descent algorithm will require $n$ partial derivatives.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
